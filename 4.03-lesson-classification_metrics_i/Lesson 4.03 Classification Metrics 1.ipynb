{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a12ce61",
   "metadata": {},
   "source": [
    "# Lesson 4.03 - Classification Metrics I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5f4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae048f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    131\n",
       "1     58\n",
       "Name: Fire-bellied toad, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and drop columns that are not required\n",
    "df = pd.read_csv(\"data/dataset.csv\", header = 1, sep = \";\")\n",
    "df = df.drop(columns=['Green frogs','Brown frogs', 'Common toad', 'Common newt', 'Great crested newt','Tree frog'])\n",
    "df = df.drop(columns=['ID', 'TR', 'VR', 'SUR1', 'SUR2', 'SUR3', 'UR', 'FR', 'RR', 'BR','MR', 'CR'])\n",
    "\n",
    "# Display the breakdown\n",
    "df['Fire-bellied toad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb65a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split and Scale Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1238ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNN\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier_y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f1c7f",
   "metadata": {},
   "source": [
    "#### Confusion Matrix output is displayed in the following sequence --> tn, fp, fn, tp\n",
    "* TN = `confusion_matrix[0][0]`\n",
    "* FP = `confusion_matrix[0][1]`\n",
    "* FN = `confusion_matrix[1][0]`\n",
    "* TP = `confusion_matrix[1][1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100a4a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41 11]\n",
      " [21  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72        52\n",
      "           1       0.21      0.12      0.16        24\n",
      "\n",
      "    accuracy                           0.58        76\n",
      "   macro avg       0.44      0.46      0.44        76\n",
      "weighted avg       0.52      0.58      0.54        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display Classification Metrics\n",
    "print(confusion_matrix(y_test, classifier_y_pred))\n",
    "print(classification_report(y_test, classifier_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329d20a",
   "metadata": {},
   "source": [
    "#### Recap on Classification Metrics Formula\n",
    "* Accuracy = True Predictions / Total Predictions\n",
    "* Precision = True Positives / (True Positives + False Positives)\n",
    "* Recall = True Positives / (True Positives + False Negatives)\n",
    "* F1-Score  = Weighted Average of Precision and Recall\n",
    "    * Offers a better overall measure of performance\n",
    "* Support = True Positives (or True Negatives) that lie in that class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
